{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This example shows text Classification as an example of supervised machine learning task.\n",
    "- The main steps are:\n",
    "    - Explore dataset\n",
    "    - Data Preparation\n",
    "    - Feature Engineering\n",
    "    - Model Training\n",
    "    - Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Definition\n",
    "- As discussed above, the problem in context is supervised learning problem. \n",
    "- The inherent requirement for supervised learning is the use of labeled dataset \n",
    "- Labeled data is necessary for the ML algorithms to learn the patterns in the data. \n",
    "- We are using a sample data from amazon reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text Classification](https://raoumer.github.io/images/MLmodel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for dataset preparation, feature engineering, model training \n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import pandas, numpy #textblob, string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore data and prepare data\n",
    "- use the sample data from amazon reviews. More data available at https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
    "- load the data into a pandas dataframe containing two columns â€“ text and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from amazon reviews\n",
    "data = open('data/corpus.txt').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is necessary to do an exploratory data analysis in order to gain some insights from the data.\n",
    "\n",
    "#### How is the data like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how many 'class label' present in the dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['__label__2', '__label__1'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the dataset balanced??\n",
    "\n",
    "If the dataset is balanced then, the dataset contains an approximately equal portion of each class.\n",
    "\n",
    "This is important, for instance, if we had two classes ( say A and B) and a 98% of observations belonging to a single class 'A', then a dumb classifier which always outputs 'A' would have 98% accuracy, even if it fail all the predictions of class B ( i.e. the minority class).\n",
    "\n",
    "Also, there are ways to cope with such inbalanced datasets, like undersample the majority class or oversample the minority one. \n",
    "Sometimes, we may have to work with imbalanced datasets. In such cases, performance metrics other than accuracy like the precision, the recall or the F1-score are preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0efd519dd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training and validation/testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode target column so that it can be used in machine learning models.\n",
    "preprocessing.LabelEncoder() is used to encode target values, i.e. y, and not the input X.\n",
    "This encodes target labels with values '0' and '1' for '__label__1' and '__label__2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "- Raw text data will be transformed into feature vectors\n",
    "- New features will be generated using the existing dataset\n",
    "- Feature Selection: selecting a subset of relevant features (i.e. predictor variables) for constructing ML model.\n",
    "- Feature Extraction: creating a new and less number of features for constructing ML model.\n",
    "    - E.g. If the original data are images. We extract the redness value, or a description of the shape of an object in the image. It's lossy, but at least we get some good representation for the ML algorithm to work.\n",
    "- Feature Selection chooses a subset of the original features whereas Feature Extraction creates new features from original features.\n",
    "- Feature engineering: careful preprocessing into more meaningful features, even if we use the original data variables or create new ones.\n",
    "    - E.g. instead of using original variables (say x, y, z) we decide to use log(x)-sqrt(y)*z instead, because this derived form is more favorable for the ML algorithm to solve the problem and get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Count Vectors as features\n",
    "- Count Vector is a matrix notation of the dataset in which:\n",
    "    - every row represents a document from the corpus, \n",
    "    - every column represents a term from the corpus, \n",
    "    - every cell represents the frequency count of a particular term in a particular document.\n",
    "    \n",
    "    \n",
    "- An example of Document Term Matrix:\n",
    "![Document Term Matrix](http://www.darrinbishop.com/wp-content/uploads/2017/10/Document-Term-Matrix.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TFIDF vectors as features\n",
    "\n",
    "- TFIDF score represents the relative importance of a term in the document and the entire corpus. \n",
    "- TFIDF score is composed by two terms: \n",
    "    - the first computes the normalized Term Frequency (TF), \n",
    "    - the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building\n",
    "\n",
    "- An importnat step in text classification is to train a classifier using the features. \n",
    "- Various machine learning classifiers:\n",
    "    - Naive Bayes Classifier\n",
    "    - Support Vector Machine\n",
    "    - Bagging Models\n",
    "    - Boosting Models\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_check(predictions_SVM, valid_y):\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    confusion_mat = confusion_matrix(predictions_SVM, valid_y)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(predictions_SVM, valid_y))\n",
    "    \n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(predictions_SVM, valid_y)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(predictions_SVM, valid_y)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(predictions_SVM, valid_y)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(predictions_SVM, valid_y)\n",
    "    print('F1 score: %f' % f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM Model\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1091  171]\n",
      " [ 168 1070]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87      1262\n",
      "           1       0.86      0.86      0.86      1238\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      2500\n",
      "   macro avg       0.86      0.86      0.86      2500\n",
      "weighted avg       0.86      0.86      0.86      2500\n",
      "\n",
      "Accuracy: 0.864400\n",
      "Precision: 0.862208\n",
      "Recall: 0.864297\n",
      "F1 score: 0.863251\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the word level tfidf\n",
    "\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(xtrain_tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(xvalid_tfidf)\n",
    "\n",
    "performance_check(predictions_SVM, valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1058  208]\n",
      " [ 201 1033]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      1266\n",
      "           1       0.83      0.84      0.83      1234\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      2500\n",
      "   macro avg       0.84      0.84      0.84      2500\n",
      "weighted avg       0.84      0.84      0.84      2500\n",
      "\n",
      "Accuracy: 0.836400\n",
      "Precision: 0.832393\n",
      "Recall: 0.837115\n",
      "F1 score: 0.834747\n"
     ]
    }
   ],
   "source": [
    "# Fit the tfidf_ngram\n",
    "\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto') # change kernels to 'poly', 'rbf' and see\n",
    "SVM.fit(xtrain_tfidf_ngram,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(xvalid_tfidf_ngram)\n",
    "\n",
    "performance_check(predictions_SVM, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "- Data pre-processing steps to improve accuracy.\n",
    "- other Word Vectorization techniques such as Word2Vec.\n",
    "- Parameter tuning with techniques such as Grid Search.\n",
    "- Other classification Algorithms Like Bagging/Boosting Models, deep learning, etc.\n",
    "- Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- https://raoumer.github.io/blog_posts/building_ML_model.html\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "- https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\n",
    "- https://towardsdatascience.com/text-classification-in-python-dd95d264c802\n",
    "- https://towardsdatascience.com/getting-data-ready-for-modelling-feature-engineering-feature-selection-dimension-reduction-39dfa267b95a\n",
    "- https://stackoverflow.com/questions/39130600/what-is-the-difference-between-feature-engineering-and-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
